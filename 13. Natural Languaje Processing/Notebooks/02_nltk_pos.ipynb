{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25925,
     "status": "ok",
     "timestamp": 1596793612246,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "M0rVNGbXUFSD",
    "outputId": "62f456ce-15b5-4e63-bd39-a859573d56ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
      "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n",
      "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
     ]
    }
   ],
   "source": [
    "# install the requirements\n",
    "!pip install nltk\n",
    "!python -m nltk.downloader book\n",
    "!python -m nltk.downloader cess_esp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "fTNqzv44UFSI"
   },
   "source": [
    "# Resumen NLTK: Etiquetado morfológico (*part-of-speech tagging*)\n",
    "\n",
    "Este resumen se corresponde con el capítulo 5 del NLTK Book [Categorizing and Tagging Words](http://www.nltk.org/book/ch05.html). La lectura del capítulo es muy recomendable.\n",
    "\n",
    "## Etiquetado morfológico con NLTK\n",
    "\n",
    "NLTK propociona varias herramientas para poder crear fácilmente etiquetadores morfológicos (*part-of-speech taggers*). Veamos algunos ejemplos.\n",
    "\n",
    "Para empezar, necesitamos importar el módulo `nltk` que nos da acceso a todas las funcionalidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "LQouLM-iUFSI"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "TfL8tbFNUFSN"
   },
   "source": [
    "Como primer ejemplo, podemos utilizar la función `nltk.pos_tag` para etiquetar morfológicamente una oración en inglés, siempre que la especifiquemos como una lista de palabras o tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 874,
     "status": "ok",
     "timestamp": 1596793816773,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "KLA8UO8BUFSO",
    "outputId": "4cda1868-1ba0-4d6f-a7f7-ca7e5d31b3ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('lost', 'JJ'), ('dog', 'NN'), ('I', 'PRP'), ('found', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('park', 'NN')]\n",
      "[('The', 'DT'), ('progress', 'NN'), ('of', 'IN'), ('the', 'DT'), ('humankind', 'NN'), ('as', 'IN'), ('I', 'PRP'), ('progress', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "oracion1 = \"This is the lost dog I found at the park\".split()\n",
    "oracion2 = \"The progress of the humankind as I progress\".split()\n",
    "print(nltk.pos_tag(oracion1))\n",
    "print(nltk.pos_tag(oracion2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "WxFTH-V_UFSR"
   },
   "source": [
    "El etiquetador funciona bastante bien aunque comete errores, obviamente. Si probamos con la famosa frase de Chomksy detectamos palabras mal etiquetadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1596793913995,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "hUG11yxyUFSS",
    "outputId": "22f54c74-b999-45a2-960e-ee6db2dccf08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Green', 'JJ'), ('colorless', 'NN'), ('ideas', 'NNS'), ('sleep', 'VBP'), ('furiously', 'RB')]\n",
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Prince', 'NNP')]\n",
      "[('He', 'PRP'), ('was', 'VBD'), ('born', 'VBN'), ('during', 'IN'), ('the', 'DT'), ('summer', 'NN'), ('of', 'IN'), ('1988', 'CD')]\n",
      "[(\"She's\", 'NNP'), (\"Tony's\", 'NNP'), ('sister', 'NN')]\n",
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Asolamatom', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('stromkupft', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "oracion3 = \"Green colorless ideas sleep furiously\".split()\n",
    "\n",
    "print(nltk.pos_tag(oracion3))\n",
    "\n",
    "print(nltk.pos_tag([\"My\", \"name\", \"is\", \"Prince\"]))\n",
    "\n",
    "print(nltk.pos_tag(\"He was born during the summer of 1988\".split()))\n",
    "\n",
    "print(nltk.pos_tag(\"\"\"She's Tony's sister\"\"\".split()))\n",
    "\n",
    "print(nltk.pos_tag(\"\"\"My name is Asolamatom and I have a stromkupft dog\"\"\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "gzM5Bq0sUFSW"
   },
   "source": [
    "¿Cómo funciona este etiquetador? `nltk.pos_tag` es un etiquetador morfológico basado en aprendizaje automático. A partir de miles de ejemplos de oraciones etiquetadas manualmente, el sistema *ha aprendido*, calculando frecuencias y generalizando cuál es la categoría gramatical más probable para cada token. \n",
    "\n",
    "Como sabes, desde NLTK podemos acceder a corpus que ya están etiquetados. Vamos a utilizar alguno de los que ya conoces, el corpus de Brown, para entrenar nuestros propios etiquetadores.\n",
    "\n",
    "Para ello importamos el corpus de Brown y almacenamos en un par de variables las noticias de este corpus en su versión etiquetada morfológicamente y sin etiquetar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "4XWdErv8UFSX"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# Cargar una lista de palabras sin ningún tipo de etiquetas\n",
    "brown_sents = brown.sents(categories=\"news\")\n",
    "\n",
    "# Las mismas listas de palabras pero etiquetadas\n",
    "brown_tagged_sents = brown.tagged_sents(categories=\"news\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "uBB2kEg2UFSa"
   },
   "source": [
    "Para comparar ambas versiones, podemos imprimir la primera oración de este corpus en su versión sin etiquetas (fíjate que se trata de una lista de tokens, sin más) y en su versión etiquetada (se trata de una lista de tuplas donde el primer elemento es el token y el segundo es la etiqueta morfológica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1596794469926,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "WpQkEJMtUFSb",
    "outputId": "0c5d6537-7a6f-41b3-a18f-74e8eb57e92e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# imprimimos la primera oración de las noticias de Brown\n",
    "print(brown_sents[0])  # sin anotar\n",
    "print(brown_tagged_sents[0])  # etiquetada morfológicamenteh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "U7XwTkVYUFSe"
   },
   "source": [
    "## Etiquetado morfológico automático\n",
    "\n",
    "### Etiquetador por defecto\n",
    "\n",
    "NLTK nos da acceso a tipos de etiquetadores morfológicos. Veamos cómo utilizar algunos de ellos.\n",
    "\n",
    "A la hora de enfrentarnos al etiquetado morfológico de un texto, podemos adoptar una estrategia sencilla consistente en etiquetar por defecto todas las palabras con la misma categoría gramatical. Con NLTK podemos utilizar un `DefaultTagger` que etiquete todos los tokens como sustantivo. Las categoría **sustantivo singular** (`NN` en el esquema de etiquetas de Treebank) suele ser la más frecuente. Veamos qué tal funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1596794777740,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "H-H18BtQUFSe",
    "outputId": "77f2e451-fd04-494a-eb61-2affe57f2977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'NN'), ('is', 'NN'), ('the', 'NN'), ('lost', 'NN'), ('dog', 'NN'), ('I', 'NN'), ('found', 'NN'), ('at', 'NN'), ('the', 'NN'), ('park', 'NN')]\n",
      "[('The', 'NN'), ('progress', 'NN'), ('of', 'NN'), ('the', 'NN'), ('humankind', 'NN'), ('as', 'NN'), ('I', 'NN'), ('progress', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "defaultTagger = nltk.DefaultTagger(\"NN\")\n",
    "\n",
    "print(defaultTagger.tag(oracion1))\n",
    "print(defaultTagger.tag(oracion2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "MXCz8llnUFSi"
   },
   "source": [
    "Obviamente no funciona bien, pero ojo, en el ejemplo anterior con `oracion1` hemos etiquetado correctamente 2 de 10 tokens. Si lo evaluamos con un corpus más grande, como el conjunto de oraciones de Brown que ya tenemos, obtenemos una precisión superior al 13%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1085,
     "status": "ok",
     "timestamp": 1596794780371,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "1iVBkU1LUFSj",
    "outputId": "6fdc3456-bc2e-4266-a121-58e486b94aae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13089484257215028"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaultTagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ENW34hJiUFSm"
   },
   "source": [
    "el método `.evaluate` que podemos ejecutar con cualquier etiquetador si especificamos como argumento una colección de referencia que ya esté etiquetada, nos devuelve un número: la **precisión**. Esta precisión se calcula como el porcentaje de tokens correctamente etiquetados por el *tagger*, teniendo en cuenta el corpus especificado como referencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ket42gShUFSn"
   },
   "source": [
    "Obviamente, los resultados son malos. Probemos con otras opciones más sofisticadas.\n",
    "\n",
    "### Etiquetador basado en Expresiones Regulares\n",
    "\n",
    "Las expresiones regulares consisten en un lenguaje formal que nos permite especificar cadenas de texto. Ya las hemos utilizado en ocasiones anteriores. Pues bien, ahora podemos probar a definir distintas categorías morfológicas a partir de patrones, al menos para fenómenos morfológicos regulares.\n",
    "\n",
    "A continuación definimos la variable `patrones` como una lista de tuplas, cuyo primer elemento se corresponde con la expresion regular que queremos capturar y el segundo elemento como la categoría gramatical. Y a partir de estas expresiones regulares creamos un `RegexpTagger`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "f02XD6S_UFSo"
   },
   "outputs": [],
   "source": [
    "patrones = [\n",
    "    (r\"[Aa]m$\", \"BEM\"),  # irregular forms of 'to be'\n",
    "    (r\"[Aa]re$\", \"BER\"),  #\n",
    "    (r\"[Ii]s$\", \"BEZ\"),  #\n",
    "    (r\"[Ww]as$\", \"BEDZ\"),  #\n",
    "    (r\"[Ww]ere$\", \"BED\"),  #\n",
    "    (r\"[Bb]een$\", \"BEN\"),  #\n",
    "    (r\"[Hh]ave$\", \"HV\"),  # irregular forms of 'to have'\n",
    "    (r\"[Hh]as$\", \"HVZ\"),  #\n",
    "    (r\"[Hh]ad$\", \"HVD\"),  #\n",
    "    (r\"I$\", \"PRP\"),  # personal pronouns\n",
    "    (r\"[Yy]ou$\", \"PRP\"),  #\n",
    "    (r\"[Hh]e$\", \"PRP\"),  #\n",
    "    (r\"[Ss]he$\", \"PRP\"),  #\n",
    "    (r\"[Ii]t$\", \"PRP\"),  #\n",
    "    (r\"[Tt]hey$\", \"PRP\"),  #\n",
    "    (r\"[Aa]n?$\", \"AT\"),  #\n",
    "    (r\"[Tt]he$\", \"AT\"),  #\n",
    "    (r\"[Ww]h.+$\", \"WP\"),  # wh- pronoun\n",
    "    (r\".*ing$\", \"VBG\"),  # gerunds\n",
    "    (r\".*ed$\", \"VBD\"),  # simple past\n",
    "    (r\".*es$\", \"VBZ\"),  # 3rd singular present\n",
    "    (r\"[Cc]an(not|n\\'t)?$\", \"MD\"),  # modals\n",
    "    (r\"[Mm]ight$\", \"MD\"),  #\n",
    "    (r\"[Mm]ay$\", \"MD\"),  #\n",
    "    (r\".+ould$\", \"MD\"),  # modals: could, should, would\n",
    "    (r\".*ly$\", \"RB\"),  # adverbs\n",
    "    (r\".*\\'s$\", \"NN$\"),  # possessive nouns\n",
    "    (r\".*s$\", \"NNS\"),  # plural nouns\n",
    "    (r\"-?[0-9]+(.[0-9]+)?$\", \"CD\"),  # cardinal numbers\n",
    "    (r\"^to$\", \"TO\"),  # to\n",
    "    (r\"^in$\", \"IN\"),  # in prep\n",
    "    (r\"^[A-Z]+([a-z])*$\", \"NNP\"),  # proper nouns\n",
    "    (r\".*\", \"NN\"),  # nouns (default)\n",
    "]\n",
    "\n",
    "regexTagger = nltk.RegexpTagger(patrones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1596795392308,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "sfAyvvRiUFSr",
    "outputId": "cc717e0a-054f-408f-dde8-b0932a2a2080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('was', 'BEDZ'), ('taking', 'VBG'), ('a', 'AT'), ('sunbath', 'NN'), ('in', 'IN'), ('Alpedrete', 'NNP')]\n",
      "[('She', 'PRP'), ('would', 'MD'), ('have', 'HV'), ('found', 'NN'), ('100', 'CD'), ('dollars', 'NNS'), ('in', 'IN'), ('the', 'AT'), ('bag', 'NN')]\n",
      "[('DSFdfdsfsd', 'NNP'), ('1852', 'CD'), ('to', 'TO'), ('dgdfgould', 'MD'), ('fXXXdg', 'NN'), ('in', 'IN'), ('XXXfdg', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "print(regexTagger.tag(\"I was taking a sunbath in Alpedrete\".split()))\n",
    "\n",
    "print(regexTagger.tag(\"She would have found 100 dollars in the bag\".split()))\n",
    "\n",
    "print(regexTagger.tag(\"DSFdfdsfsd 1852 to dgdfgould fXXXdg in XXXfdg\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "qNxkV8WSUFSv"
   },
   "source": [
    "Cuando probamos a evaluarlo con un corpus de oraciones más grande, vemos que nuestra precisión sube por encima del 32%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4463,
     "status": "ok",
     "timestamp": 1596795402917,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "cShi0cLAUFSw",
    "outputId": "28d3472a-6e3c-4b2d-9010-b0c039d0685c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35668397080175823"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexTagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "CGSlPpByUFSz"
   },
   "source": [
    "### Etiquetado basado en ngramas\n",
    "\n",
    "Antes hemos dicho que la función `nltk.pos_tag` tenía un etiquetador morfológico que funcionaba con información estadística. A continuación vamos a reproducir, a pequeña escala, el proceso de entrenamiento de un etiquetador morfológico basado en aprendizaje automático. \n",
    "\n",
    "En general, los sistemas de aprendizaje automático funcionan del siguiente modo:\n",
    "\n",
    "* Se etiqueta manualmente una colección de datos. Cuanto más grande y más representativa sea la colección, mejores datos podremos extraer.\n",
    "* El sistema de aprendizaje procesa la colección de entrenamiento y \"aprende\" a partir de los ejemplos observados. En el caso del etiquetado morfológico, el sistema calcula frecuencias y generaliza (de distintas maneras) cuáles son las categorías gramaticales más probables para cada palabra. \n",
    "* Una vez que el sistema ha sido entrenado, se evalúa su rendimiento sobre datos no observados.\n",
    "\n",
    "Nosotros tenemos un pequeño corpus de ejemplos etiquetados: las oraciones del corpus de Brown de la categoría \"noticias\". Lo primero que necesitamos hacer es separar nuestros corpus de entrenamiento y test. En este caso, vamos a reservar el primer 90% de las oraciones para el entrenamiento (serán los ejemplos observados a partir de los cuales nuestro etiquetador aprenderá) y vamos a dejar el 10% restante para comprobar qué tal funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1527,
     "status": "ok",
     "timestamp": 1596795987230,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "U065Sj9AUFSz",
    "outputId": "9de17d1a-88bf-4c8b-ff9f-da8ae9939591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4623\n",
      "4160.7\n"
     ]
    }
   ],
   "source": [
    "print(len(brown_tagged_sents))\n",
    "\n",
    "print((len(brown_tagged_sents) * 90) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1596796022594,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "IdZcIRaSUFS2",
    "outputId": "cf1c6c6d-1f9a-4418-b4e1-96b83f45207f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "[('But', 'CC'), ('in', 'IN'), ('all', 'ABN'), ('its', 'PP$'), ('175', 'CD'), ('years', 'NNS'), (',', ','), ('not', '*'), ('a', 'AT'), ('single', 'AP'), ('Negro', 'NP'), ('student', 'NN'), ('has', 'HVZ'), ('entered', 'VBN'), ('its', 'PP$'), ('classrooms', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "size = int(len(brown_tagged_sents) * 0.9)  # asegúrate de que conviertes esto a entero\n",
    "corpusEntrenamiento = brown_tagged_sents[:size]\n",
    "corpusTest = brown_tagged_sents[size:]\n",
    "\n",
    "# como ves, estos corpus contienen oraciones diferentes\n",
    "print(corpusEntrenamiento[0])\n",
    "print(corpusTest[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "kM2-Sh7RUFS6"
   },
   "source": [
    "A continuación vamos a crear un etiquetador basado en unigramas (secuencias de una palabra o palabras sueltas) a través de la clase `nltk.UnigramTagger`, proporcionando nuestro `corpusEntrenamiento` para que aprenda. Una vez entrenado, vamos a evaluar su rendimiento sobre `corpusTest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1418,
     "status": "ok",
     "timestamp": 1596796130513,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "1G0C2h1oUFS7",
    "outputId": "22b8a7d1-b384-44ba-ffa9-22be914a76a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8121200039868434\n"
     ]
    }
   ],
   "source": [
    "unigramTagger = nltk.UnigramTagger(corpusEntrenamiento)\n",
    "print(unigramTagger.evaluate(corpusTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 705,
     "status": "ok",
     "timestamp": 1596796147837,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "zS560oAQUFS9",
    "outputId": "1c972251-5bb8-4554-af15-4d91bd28509a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'BEZ'), ('the', 'AT'), ('lost', 'VBD'), ('dog', 'NN'), ('I', 'PPSS'), ('found', 'VBN'), ('at', 'IN'), ('the', 'AT'), ('park', 'NN')]\n",
      "[('The', 'AT'), ('progress', 'NN'), ('of', 'IN'), ('the', 'AT'), ('humankind', None), ('as', 'CS'), ('I', 'PPSS'), ('progress', 'NN')]\n",
      "[('Green', 'JJ-TL'), ('colorless', None), ('ideas', 'NNS'), ('sleep', None), ('furiously', None)]\n"
     ]
    }
   ],
   "source": [
    "# ¿qué tal se etiquetan nuestras oraciones de ejemplo?\n",
    "print(unigramTagger.tag(oracion1))\n",
    "print(unigramTagger.tag(oracion2))\n",
    "print(unigramTagger.tag(oracion3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "zL3BHFfOUFTB"
   },
   "source": [
    "Los etiquetadores basados en unigramas se construyen a partir del simple cálculo de una distribución de frecuencia para cada token (palabra) y asignan siempre la etiqueta morfológica más probable. En nuestro caso, esta estrategia funciona relativamente bien: el tagger supera el 81% de precisión. Sin embargo, esta aproximación presenta numerosos problemas a la hora de etiquetar palabras homógrafas (un mismo token funcionando con más de una categoría gramatical). Si probamos con nuestra `oracion2`, comprobamos que la segunda aparición de *progress* no es etiquetada correctamente.\n",
    "\n",
    "Intuitivamente, podemos pensar que sabríamos distinguir ambas categorías si tuviéramos en cuenta algo del contexto de aparición de las palabras: *progress* es un sustantivo cuando aparece después del artículo *the* y es verbo cuando aparece tras un pronombre personal como *I*. Si en lugar de calcular frecuencias de unigramas, extendiéramos los cálculos a secuencias de dos o tres palabras, podríamos tener mejores resultados. Y precisamente por eso vamos a calcular distribuciones de frecuencias condicionales: asignaremos a cada token la categoría gramatical más frecuente teniendo en cuenta la categoría gramatical de la(s) palabra(s) inmediatamente anterior(es). \n",
    "\n",
    "Creamos un par de etiquetadores basado en bigramas (secuencias de dos palabras) o trigramas (secuencias de tres palabras) a través de las clases `nltk.BigramTagger` y `nltk.TrigramTagger`. Y los probamos con nuestra `oracion2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "2-KzvQs9UFTC"
   },
   "outputs": [],
   "source": [
    "bigramTagger = nltk.BigramTagger(corpusEntrenamiento)\n",
    "\n",
    "trigramTagger = nltk.TrigramTagger(corpusEntrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1596796554821,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "B1D3Oq3_UFTF",
    "outputId": "b13226e7-5aa9-42db-cb35-54f5175d6539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('progress', None), ('of', None), ('the', None), ('humankind', None), ('as', None), ('I', None), ('progress', None)]\n",
      "[('The', 'AT'), ('progress', None), ('of', None), ('the', None), ('humankind', None), ('as', None), ('I', None), ('progress', None)]\n"
     ]
    }
   ],
   "source": [
    "# funciona fatal :-(\n",
    "print(bigramTagger.tag(oracion2))\n",
    "print(trigramTagger.tag(oracion2))\n",
    "\n",
    "# aquí hago trampas, le pido que analice una oración que ya ha visto durante el entrenamiento\n",
    "# print(bigramTagger.tag(['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of',\n",
    "# \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "4MJ9_FbiUFTJ"
   },
   "source": [
    "Como se ve en los ejemplos, los resultados son desastrosos. La mayoría de los tokens se quedan sin etiqueta y se muestran como `None`.\n",
    "\n",
    "Si los evaluamos con nuestra colección de test, vemos que apenas superan el 10% de precisión. Peores resultados que nuestro `DefaultTagger`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 674,
     "status": "ok",
     "timestamp": 1596796562188,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "3RMXRGUtUFTK",
    "outputId": "e0da642d-33e2-446b-a24f-511fedad5d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10206319146815508\n",
      "0.0626931127279976\n"
     ]
    }
   ],
   "source": [
    "print(bigramTagger.evaluate(corpusTest))\n",
    "print(trigramTagger.evaluate(corpusTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "7xG5Yd05UFTO"
   },
   "source": [
    "¿Por qué ocurre esto? La intuición no nos engaña, y es verdad que si calculamos distribuciones de frecuencia condicionales teniendo en cuenta secuencias de palabras más largas, nuestros datos serán más finos. Sin embargo, cuando consideramos secuencias de tokens más largos nos arriesgamos a que dichas secuencias no aparezcan como tales en el corpus de entrenamiento. \n",
    "\n",
    "En el ejemplo de `oracion2`, nuestro `bigramTagger` es incapaz de etiquetar la palabra *progress* porque no ha encontrado en el corpus de entrenamiento ni el bigrama (**The, progress**) ni (**I, progress**). Obviamente, nuestro `trigramTagger` tampoco ha encontrado los trigramas (**`INICIO_DE_ORACIÓN`, The, progress**) o (**as, I, progress**). Si esas secuencias no aparecen en el corpus de entrenamiento, no hay nada que aprender. \n",
    "\n",
    "\n",
    "\n",
    "### Combinamos etiquetadores\n",
    "\n",
    "En estos casos, la solución más satisfactoria consiste en combinar de manera incremental la potencia de todos nuestros etiquetadores. Vamos a crear nuevos taggers que utilicen otros como respaldo. \n",
    "\n",
    "Utilizaremos un tagger de trigramas que, cuando no tenga respuesta para etiquetar un determinado token, utilizará como respaldo el tagger de bigramas. A su vez, el tagger de bigramas tirará del de unigramas cuando no tenga respuesta. Por último, el de unigramas utilizará como respaldo el tagger de expresiones regulares que definimos antes. De esta manera aumentamos la precisión hasta casi el 87%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "kRcEJ4WuUFTO"
   },
   "outputs": [],
   "source": [
    "# creamos los etiquetadores\n",
    "unigramTagger = nltk.UnigramTagger(corpusEntrenamiento, backoff=regexTagger)\n",
    "bigramTagger = nltk.BigramTagger(corpusEntrenamiento, backoff=unigramTagger)\n",
    "trigramTagger = nltk.TrigramTagger(corpusEntrenamiento, backoff=bigramTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1180,
     "status": "ok",
     "timestamp": 1596797026672,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "fujU851UUFTR",
    "outputId": "5fcf125c-ec4a-4190-c66b-bb4f7af9f199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8585667297916875\n",
      "0.8679358118209908\n",
      "0.8656433768563739\n"
     ]
    }
   ],
   "source": [
    "# ¿qué precisión tienen?\n",
    "print(unigramTagger.evaluate(corpusTest))\n",
    "print(bigramTagger.evaluate(corpusTest))\n",
    "print(trigramTagger.evaluate(corpusTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 847,
     "status": "ok",
     "timestamp": 1596797034503,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "BnjOtwr5UFTU",
    "outputId": "a63f77a7-1065-4261-8ba7-415554571718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'BEZ'), ('the', 'AT'), ('lost', 'VBD'), ('dog', 'NN'), ('I', 'PPSS'), ('found', 'VBD'), ('at', 'IN'), ('the', 'AT'), ('park', 'NN')]\n",
      "[('The', 'AT'), ('progress', 'NN'), ('of', 'IN'), ('the', 'AT'), ('humankind', 'NN'), ('as', 'CS'), ('I', 'PPSS'), ('progress', 'NN')]\n",
      "[('Green', 'JJ-TL'), ('colorless', 'NNS'), ('ideas', 'NNS'), ('sleep', 'NN'), ('furiously', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# ¿qué tal funcionan?\n",
    "print(trigramTagger.tag(oracion1))\n",
    "print(trigramTagger.tag(oracion2))\n",
    "print(trigramTagger.tag(oracion3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "qBkKR2QzUFTY"
   },
   "source": [
    "### Ejercicio 1: mejorando nuestro tagger\n",
    "\n",
    "Vamos a mejorar nuestro etiquetador extendiendo el corpus de entrenamiento. En lugar de usar solo los textos de noticias, vamos a utilizar el corpus de Brown completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1596797998965,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "-UeObn02k7Su",
    "outputId": "90002418-dac5-4006-bf89-b4b024f8bc09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_tagged_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55778,
     "status": "ok",
     "timestamp": 1596798490522,
     "user": {
      "displayName": "Felipe Fernandez pomar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAIBB1UUNahiuBW35nhzx_0hZxW-dTtBh-sbrK0Q=s64",
      "userId": "06620814048662575791"
     },
     "user_tz": -120
    },
    "id": "UjjTeObKUFTY",
    "outputId": "bf35c8e8-9af4-4bec-9b5a-2cbef25ede86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.906126625452995\n",
      "0.9276145811127692\n",
      "0.9277595395438073\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import random\n",
    "\n",
    "# cargamos el corpus y hacemos un shuffle a la lista de oraciones para eliminar sesgos de género\n",
    "corpus = list(brown.tagged_sents())\n",
    "random.shuffle(corpus)\n",
    "\n",
    "# split entre training y size\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "training = corpus[:size]\n",
    "test = corpus[size:]\n",
    "\n",
    "# etiquetadores\n",
    "unigramTagger = nltk.UnigramTagger(training, backoff = regexTagger)\n",
    "bigramTagger = nltk.BigramTagger(training, backoff = unigramTagger)\n",
    "trigramTagger = nltk.TrigramTagger(training, backoff = bigramTagger)\n",
    "\n",
    "print(unigramTagger.evaluate(test))\n",
    "print(bigramTagger.evaluate(test))\n",
    "print(trigramTagger.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "4XDiZHT3UFTc"
   },
   "source": [
    "### Ejercicio 2: tagger en castellano\n",
    "\n",
    "[Entrenando un etiquetador morfológico en castellano](pos-tagger-es.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02-nltk-pos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
